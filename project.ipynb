{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool, Queue\n",
    "import lxml\n",
    "import tldextract\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from contextlib import ExitStack\n",
    "from typing import Generator, Dict, Any\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import GermanStemmer, EnglishStemmer, RussianStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from time import time\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_symbols = re.compile(r'[^a-zA-Z0-9а-яА-Я\\u00E4\\u00F6\\u00FC\\u00C4\\u00D6\\u00DC\\u00df]')\n",
    "stopwords_new = ['http', 'url', 'img','html', 'https', 'org', 'www', 'jpg', 'png', 'net','com','php', 'uid','src', 'ahttp', 'index', 'htm']\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords_new) + r')\\b\\s*')\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "musor = re.compile(r'\\d{1}(\\w{2})\\d{2,4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_ru = RussianStemmer()\n",
    "stemmer_eng = EnglishStemmer()\n",
    "stemmer_ger = GermanStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(['english', 'russian', 'german']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_position_start = 1\n",
    "my_position_end = 28027\n",
    "queue = Queue() # очередь ссылок на книги\n",
    "for i in [426, 1393, 3345, 3800, 6085, 24245]:\n",
    "    queue.put(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split title to words\n",
    "def split_title(title):\n",
    "    words = nltk.word_tokenize(title)\n",
    "    without_extra_words = [stemmer_ru.stem(stemmer_eng.stem(stemmer_ger.stem(word))) for word in words if word not in stop_words]\n",
    "    return without_extra_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split text to words\n",
    "def split_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    without_extra_words = [stemmer_ru.stem(stemmer_eng.stem(stemmer_ger.stem(word))) for word in words if word not in stop_words]\n",
    "    return without_extra_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns words which title contains and text contains\n",
    "def get_content(title, text, n=None):\n",
    "    title_words = split_title(title)\n",
    "    text_words = split_text(text)\n",
    "    if n is not None:\n",
    "        vectorizer = CountVectorizer().fit(text_words)\n",
    "        tmp_words_matrix = vectorizer.transform(text_words)\n",
    "        tmp_words_count = np.sum(tmp_words_matrix, axis=0)\n",
    "        tmp_words = [(word, tmp_words_count[0, ind]) for word, ind in vectorizer.vocabulary_.items() if len(word) > 2]\n",
    "        tmp_words = sorted(tmp_words, reverse=True, key=lambda x: x[1])\n",
    "        tmp_words = [word[0] for word in tmp_words[:n]]\n",
    "        doc_words = title_words + tmp_words\n",
    "    else:\n",
    "        doc_words = title_words + text_words\n",
    "    return doc_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(number_of_doc):\n",
    "    with open('./content/{:d}.dat'.format(number_of_doc), encoding='utf-8') as file:\n",
    "        text_checker = {}\n",
    "        html_test = file.read()\n",
    "        soup = BeautifulSoup(html_test, 'html.parser')\n",
    "        if soup.title:\n",
    "            title_name = soup.title.text\n",
    "            title_name = regex_symbols.sub(\" \", title_name)\n",
    "            title_name = re.sub(\"\\s\\s+\" , \" \", title_name)\n",
    "        else:\n",
    "            title_name = ' '\n",
    "        url = soup.text[:soup.text.index('\\n')]\n",
    "        url = tldextract.extract(url)\n",
    "        url = url.domain + '.' + url.suffix\n",
    "        text = soup.text[soup.text.index('\\n'):].lower()\n",
    "        text = regex_symbols.sub(\" \", text)\n",
    "        text = shortword.sub(\" \", text)\n",
    "        text = pattern.sub(\" \", text)\n",
    "        text = musor.sub(' ', text)\n",
    "        text = re.sub(\"\\s\\s+\" , \" \", text)\n",
    "        text = re.sub('\\xa0|\\xad', ' ', text)\n",
    "        content = get_content(title_name, text, n=25)\n",
    "        text_checker[number_of_doc] = [url] + content\n",
    "        return text_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.83it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_all_docs(i):\n",
    "    with gzip.open('data_bad/part_{:05d}.jsonl.gz'.format(i), mode='wb') as f_json:\n",
    "        f_json = codecs.getwriter('utf8')(f_json)\n",
    "\n",
    "        while not queue.empty():\n",
    "            try:\n",
    "                id_new = queue.get()\n",
    "                record = process_doc(id_new)\n",
    "            except Exception as e:\n",
    "                print(id_new, file=sys.stderr)\n",
    "                print(e, file=sys.stderr)\n",
    "            record_str = json.dumps(record, ensure_ascii=False)\n",
    "            print(record_str, file=f_json)\n",
    "\n",
    "            # счетчик должен атомарно обновиться\n",
    "            with lock:\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "with Pool(processes=8) as pool, tqdm(total=queue.qsize()) as pbar:\n",
    "    lock = pbar.get_lock()\n",
    "    pool.map(process_all_docs, range(pool._processes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def records_reader(dirname: str) -> Generator[Dict[str, Any], None, None]:\n",
    "    with ExitStack() as stack:\n",
    "        files = [stack.enter_context(gzip.open(dirname + '/' + i, mode='rb')) for i in tqdm(listdir(dirname))]\n",
    "        for j in files:\n",
    "            d = codecs.getreader('utf8')(j)\n",
    "            for k in d:\n",
    "                yield json.loads(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 160.42it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(records_reader('data_bad'))\n",
    "#df.to_csv('prom_res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2073, 5120, 10114, 14030, 19562, 24245}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([i for i in range(1, 28027)]) - set(np.unique(df.columns.values.astype('int')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(columns=['id', 'words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, words]\n",
       "Index: []"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicter = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    for j, k in df.loc[i][~df.loc[i].isna()].items():\n",
    "        dicter[str(j)] = ' '.join(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5120': 'nukucihumun.tk сумм подотчетн лиц организац счет работник отчет расход расчет средств денежн наличн выда учет выдач командировк котор документ авансов сотрудник',\n",
       " '10114': 'onliner.by memb профил пользовател лет ден офлайн сайт техник рожден город отправ личн сообщен август пленк senior окн средств neman очистк',\n",
       " '19562': 'mysonce.ru скача gta samp e торрент gta samp скача игр san andrea торрент multiplay сервер верс как папк файл sa da кача гта grand theft auto',\n",
       " '14030': 'onliner.by объявлен memb год пользовател autoc senior масл техник офлайн сайт ден рожден город профил отправ личн сообщен лет цен merc',\n",
       " '24245': 'sci-article.ru анализ нарушен иммунологическ реактивн у дет с заболеван почек дет cd лимфоцит кров хроническ групп иммунологическ показател гломерулонефрит воспалительн процесс стат активац ig анализ помощ иммун корреляцион числ сывороточн'}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>zrenielib.ru м б аншин центр репродукц генетик...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>nashizubki.ru современ стоматолог кто так стом...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>yaplakal.com есл счетчик яплакал счетчик плат ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>det-sad45.ru прошивк dexp инструкц прошивк dex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>tks.ru медицинск издел формулировк нов ру арх ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28020</th>\n",
       "      <td>5120</td>\n",
       "      <td>nukucihumun.tk сумм подотчетн лиц организац сч...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28021</th>\n",
       "      <td>10114</td>\n",
       "      <td>onliner.by memb профил пользовател лет ден офл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28022</th>\n",
       "      <td>19562</td>\n",
       "      <td>mysonce.ru скача gta samp e торрент gta samp с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28023</th>\n",
       "      <td>14030</td>\n",
       "      <td>onliner.by объявлен memb год пользовател autoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28024</th>\n",
       "      <td>24245</td>\n",
       "      <td>sci-article.ru анализ нарушен иммунологическ р...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28025 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              words\n",
       "0          1  zrenielib.ru м б аншин центр репродукц генетик...\n",
       "1         17  nashizubki.ru современ стоматолог кто так стом...\n",
       "2         18  yaplakal.com есл счетчик яплакал счетчик плат ...\n",
       "3         23  det-sad45.ru прошивк dexp инструкц прошивк dex...\n",
       "4         25  tks.ru медицинск издел формулировк нов ру арх ...\n",
       "...      ...                                                ...\n",
       "28020   5120  nukucihumun.tk сумм подотчетн лиц организац сч...\n",
       "28021  10114  onliner.by memb профил пользовател лет ден офл...\n",
       "28022  19562  mysonce.ru скача gta samp e торрент gta samp с...\n",
       "28023  14030  onliner.by объявлен memb год пользовател autoc...\n",
       "28024  24245  sci-article.ru анализ нарушен иммунологическ р...\n",
       "\n",
       "[28025 rows x 2 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nukucihumun.tk сумм подотчетн лиц организац счет работник отчет расход расчет средств денежн наличн выда учет выдач командировк котор документ авансов сотрудник'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dicter.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[28024] = [int(list(dicter.keys())[4])] + [list(dicter.values())[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, k in enumerate(dicter.items()):\n",
    "    df1.loc[j] = [int(k[0])] + [k[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('train_groups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.rename(columns={'id': 'doc_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df2, df1, how='left', on='doc_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('test_groups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'pair_id', 'group_id', 'doc_id', 'target', 'words'], dtype='object')"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('test_groups.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('test_groups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./best_data/train_groups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./best_data/test_groups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "featss = [i for i in df_train.columns.values][5:] #in case features are in test/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(featss,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(featss,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mas = tf_vect.fit_transform(df_train.words.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_mas = tf_vect.fit_transform(df_merged.words.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "help_mas = np.empty((df_train.shape[0], 25*5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help_mas_merged = np.empty((df_merged.shape[0], 25*5)) # for merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_train.groupby('group_id'):\n",
    "    ind = i[1].index.values\n",
    "    values = train_mas[ind, :]\n",
    "    distances = np.sort(pairwise_distances(values, metric='cosine'), axis=1)[:,1:26]\n",
    "    #distances = np.partition(pairwise_distances(values, metric='cosine'),25, axis=1)[:,:25]\n",
    "    meds = np.tile(np.median(distances, axis=0), distances.shape[0]).reshape(distances.shape[0], -1)\n",
    "    maxs = np.tile(np.max(distances, axis=0), distances.shape[0]).reshape(distances.shape[0], -1)\n",
    "    disps = np.tile(np.std(distances, axis=0), distances.shape[0]).reshape(distances.shape[0], -1)\n",
    "    means = np.tile(np.mean(distances, axis=0), distances.shape[0]).reshape(distances.shape[0], -1)\n",
    "    help_mas[ind, :] = np.concatenate((distances, meds, maxs,disps, means), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for merged\n",
    "'''\n",
    "for i in df_merged.groupby('group_id'):\n",
    "    ind = i[1].index.values\n",
    "    values = merged_mas[ind, :]\n",
    "    distances = np.partition(pairwise_distances(values, metric='cosine'),25, axis=1)[:,:25]\n",
    "    meds = np.tile(np.median(distances, axis=0), distances.shape[0]).reshape(distances.shape[0], -1)\n",
    "    maxs = np.tile(np.max(distances, axis=0), distances.shape[0]).reshape(distances.shape[0], -1)\n",
    "    disps = np.tile(np.std(distances, axis=0), distances.shape[0]).reshape(distances.shape[0], -1)\n",
    "    means = np.tile(np.mean(distances, axis=0), distances.shape[0]).reshape(distances.shape[0], -1)\n",
    "    help_mas_merged[ind, :] = np.concatenate((distances, meds, maxs,disps, means), axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_help_merged = pd.DataFrame(help_mas_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_help_merged.columns = ['fit' + str(i) for i in df_help_merged.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_help = pd.DataFrame(help_mas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = pd.merge(df_train, df_help_merged.loc[df_train.index.values] ,how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = pd.merge(df_test, df_help_merged.loc[df_test.index.values] ,how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test.to_csv('test_groups1.csv', index=False)\n",
    "#df_train.to_csv('train_groups1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_help.columns = ['fit' + str(i) for i in df_help.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, df_help ,how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train_groups_.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mas = tf_vect.transform(df_test.words.values)\n",
    "help_mas1 = np.empty((df_test.shape[0], 25*5))\n",
    "for i in df_test.groupby('group_id'):\n",
    "    ind = i[1].index.values\n",
    "    values = test_mas[ind, :]\n",
    "    distances = np.sort(pairwise_distances(values, metric='cosine'), axis=1)[:,1:26]\n",
    "    #distances = np.partition(pairwise_distances(values, metric='cosine'),25, axis=1)[:,:25]\n",
    "    meds = np.tile(np.median(distances, axis=0), distances.shape[0]).reshape(distances.shape[0], -1)\n",
    "    maxs = np.tile(np.max(distances, axis=0), distances.shape[0]).reshape(distances.shape[0], -1)\n",
    "    disps = np.tile(np.std(distances, axis=0), distances.shape[0]).reshape(distances.shape[0], -1)\n",
    "    means = np.tile(np.mean(distances, axis=0), distances.shape[0]).reshape(distances.shape[0], -1)\n",
    "    help_mas1[ind, :] = np.concatenate((distances, meds, maxs,disps, means), axis=1)\n",
    "df_help = pd.DataFrame(help_mas1)\n",
    "df_help.columns = ['fit' + str(i) for i in df_help.columns.values]\n",
    "df_test = pd.merge(df_test, df_help ,how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('test_groups_.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_groups_.csv')\n",
    "traingroups_titledata = {}\n",
    "my_feat = [i for i in df_train.columns.values][5:]\n",
    "for i in range(len(df_train)):\n",
    "    new_doc = df_train.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    features = new_doc[my_feat]\n",
    "    title = new_doc['words']\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, features, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11689, 150) (11689,) (11689,)\n"
     ]
    }
   ],
   "source": [
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, features, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, feat_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_train.append(sorted(all_dist, reverse=True)[0:25] + list(features)    )\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test_groups_.csv')\n",
    "testgroups_titledata = {}\n",
    "for i in range(len(df_test)):\n",
    "    new_doc = df_test.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    features = new_doc[my_feat]\n",
    "    title = new_doc['words']\n",
    "    if doc_group not in testgroups_titledata:\n",
    "        testgroups_titledata[doc_group] = []\n",
    "    testgroups_titledata[doc_group].append((doc_id, title, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 150) (16627,)\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "groups_test = []\n",
    "for new_group in testgroups_titledata:\n",
    "    docs = testgroups_titledata[new_group]\n",
    "    for k, (doc_id, title, features) in enumerate(docs):\n",
    "        groups_test.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, feat_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_test.append(sorted(all_dist, reverse=True)[0:25] + list(features)   )\n",
    "X_test = np.array(X_test)\n",
    "groups_test = np.array(groups_test)\n",
    "print (X_test.shape, groups_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.concatenate((X_train, groups_train.reshape((-1,1))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "def validate_model(estimator, params, validate_param, vals):\n",
    "    score = 0\n",
    "    for t in tqdm(vals):\n",
    "        params[validate_param] = t\n",
    "        tempScore = 0\n",
    "        for train_split, test_split in kf.split(X_train, y_train):\n",
    "            X_tr = X_train[train_split]\n",
    "            y_tr = y_train[train_split]\n",
    "            X_tst = X_train[test_split]\n",
    "            y_tst = y_train[test_split]\n",
    "            clf = estimator(**params)\n",
    "            res = np.zeros((X_tst.shape[0],))\n",
    "            #for i in np.unique(X_tr[:, 150]):\n",
    "            #    ind = np.where(X_tr[:, 150] == i)\n",
    "            #    clf.fit(X_tr[ind, :150][0], y_tr[ind])\n",
    "            #    ind_test = np.where(X_tst[:, 150] == i)\n",
    "            #    res[ind_test] = clf.predict(X_tst[ind_test, :150][0])[:]\n",
    "            clf.fit(X_tr, y_tr)\n",
    "            #res = res.reshape((-1,1))\n",
    "            res = clf.predict(X_tst)\n",
    "            tempScore += f1_score(y_tst, res)\n",
    "        if (tempScore > score):\n",
    "            score = tempScore\n",
    "            result = t\n",
    "    print(validate_param, ' = ', result, ' score= ', score / 5)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:39<00:00,  9.93s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_score  =  0.5555944444444445  score=  0.7945532543681821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:51<00:00,  6.94s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators  =  30  score=  0.798112826423851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:37<00:00,  3.78s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_lambda  =  0.11108888888888889  score=  0.796012397610137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:37<00:00,  3.79s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight  =  2.4444444444444446  score=  0.8049516691405166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:37<00:00,  3.78s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha  =  0.4445  score=  0.8038768782035449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:39<00:00,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_child_weight  =  0.0001  score=  0.8022773895448502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "params = {'objective': 'binary:logistic'}\n",
    "params['base_score'] = validate_model(xgb.XGBClassifier, params, 'base_score', np.linspace(0.0001,0.99999,10))\n",
    "params['n_estimators'] = validate_model(xgb.XGBClassifier, params, 'n_estimators', \n",
    "                                            range(20,100,5))\n",
    "params['reg_lambda'] = validate_model(xgb.XGBClassifier, params, 'reg_lambda', \n",
    "                                            np.linspace(0.0001,0.999,10))\n",
    "params['scale_pos_weight'] = validate_model(xgb.XGBClassifier, params, 'scale_pos_weight', \n",
    "                                            np.linspace(2.0,3.0,10))\n",
    "params['alpha'] = validate_model(xgb.XGBClassifier, params, 'alpha', \n",
    "                                            np.linspace(0.0001,1.0,10))\n",
    "params['min_child_weight'] = validate_model(xgb.XGBClassifier, params, 'min_child_weight', \n",
    "                                            np.linspace(0.0001,1.0,10))\n",
    "paramsLogistic = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:52<00:00,  7.05s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators  =  45  score=  0.8017700095163883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:34<00:00,  3.42s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colsample_bytree  =  0.7770222222222222  score=  0.7979186909992315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:44<00:00,  4.46s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight  =  2.111111111111111  score=  0.806739480005928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:44<00:00,  4.40s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha  =  0.33340000000000003  score=  0.8072554936948716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:44<00:00,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_child_weight  =  0.7778  score=  0.8095652673115138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "params = {'booster': 'gbtree'}\n",
    "params['n_estimators'] = validate_model(xgb.XGBClassifier, params, 'n_estimators', \n",
    "                                            range(20,100,5))\n",
    "params['colsample_bytree'] = validate_model(xgb.XGBClassifier, params, 'colsample_bytree', \n",
    "                                            np.linspace(0.0001,0.999,10))\n",
    "params['scale_pos_weight'] = validate_model(xgb.XGBClassifier, params, 'scale_pos_weight', \n",
    "                                            np.linspace(2.0,3.0,10))\n",
    "params['alpha'] = validate_model(xgb.XGBClassifier, params, 'alpha', \n",
    "                                            np.linspace(0.0001,1.0,10))\n",
    "params['min_child_weight'] = validate_model(xgb.XGBClassifier, params, 'min_child_weight', \n",
    "                                            np.linspace(0.0001,1.0,10))\n",
    "paramsGbTree = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7997837788428273\n"
     ]
    }
   ],
   "source": [
    "for t in [15]:\n",
    "    tempScore = 0\n",
    "    for train_split, test_split in kf.split(X_train, y_train):\n",
    "        X_tr = X_train[train_split]\n",
    "        y_tr = y_train[train_split]\n",
    "        X_tst = X_train[test_split]\n",
    "        y_tst = y_train[test_split]\n",
    "        clf = xgb.XGBClassifier(**paramsGbTree).fit(X_tr, y_tr)\n",
    "        clf_bag = BaggingClassifier(base_estimator=clf,\n",
    "                        n_estimators=t, random_state=0).fit(X_tr, y_tr)\n",
    "        res = clf.predict(X_tst)\n",
    "        tempScore += f1_score(y_tst, res)\n",
    "    print(tempScore / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8167832167832167"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "f1_score(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8148148148148149"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = xgb.XGBClassifier(**paramsGbTree)\n",
    "clf = BaggingClassifier(base_estimator=clf1,\n",
    "                        n_estimators=15, random_state=0).fit(X_train, y_train)\n",
    "f1_score(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.0001, base_score=0.5, booster='gbtree',\n",
       "              colsample_bylevel=1, colsample_bynode=1,\n",
       "              colsample_bytree=0.22207777777777776, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=0.8889, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=35, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=9.99999975e-05, reg_lambda=1, scale_pos_weight=2.0,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster': 'gbtree',\n",
       " 'n_estimators': 35,\n",
       " 'colsample_bytree': 0.22207777777777776,\n",
       " 'scale_pos_weight': 2.0,\n",
       " 'alpha': 0.0001,\n",
       " 'min_child_weight': 0.8889}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramsGbTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.0001, base_score=0.5, booster='gbtree',\n",
       "              colsample_bylevel=1, colsample_bynode=1,\n",
       "              colsample_bytree=0.22207777777777776, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=0.8889, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=35, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=9.99999975e-05, reg_lambda=1, scale_pos_weight=2.0,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(**paramsGbTree)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = xgb.XGBClassifier(**paramsGbTree)\n",
    "clf = BaggingClassifier(base_estimator=clf1,\n",
    "                        n_estimators=15, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 150)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['target'] = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>target</th>\n",
       "      <th>words</th>\n",
       "      <th>fit0</th>\n",
       "      <th>fit1</th>\n",
       "      <th>fit2</th>\n",
       "      <th>fit3</th>\n",
       "      <th>fit4</th>\n",
       "      <th>...</th>\n",
       "      <th>fit115</th>\n",
       "      <th>fit116</th>\n",
       "      <th>fit117</th>\n",
       "      <th>fit118</th>\n",
       "      <th>fit119</th>\n",
       "      <th>fit120</th>\n",
       "      <th>fit121</th>\n",
       "      <th>fit122</th>\n",
       "      <th>fit123</th>\n",
       "      <th>fit124</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15731</td>\n",
       "      <td>0</td>\n",
       "      <td>automn.ru ваз зам подшипник ступиц нив подшипн...</td>\n",
       "      <td>0.112565</td>\n",
       "      <td>0.128736</td>\n",
       "      <td>0.303646</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.515635</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848174</td>\n",
       "      <td>0.852419</td>\n",
       "      <td>0.855621</td>\n",
       "      <td>0.859238</td>\n",
       "      <td>0.862404</td>\n",
       "      <td>0.865251</td>\n",
       "      <td>0.867542</td>\n",
       "      <td>0.870943</td>\n",
       "      <td>0.874038</td>\n",
       "      <td>0.876732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14829</td>\n",
       "      <td>0</td>\n",
       "      <td>tiu.ru ваз опт соч сравн цен куп потребительск...</td>\n",
       "      <td>0.361563</td>\n",
       "      <td>0.367733</td>\n",
       "      <td>0.445630</td>\n",
       "      <td>0.476053</td>\n",
       "      <td>0.587270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848174</td>\n",
       "      <td>0.852419</td>\n",
       "      <td>0.855621</td>\n",
       "      <td>0.859238</td>\n",
       "      <td>0.862404</td>\n",
       "      <td>0.865251</td>\n",
       "      <td>0.867542</td>\n",
       "      <td>0.870943</td>\n",
       "      <td>0.874038</td>\n",
       "      <td>0.876732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15764</td>\n",
       "      <td>0</td>\n",
       "      <td>drom.ru куп ступиц лад калин трансмисс переход...</td>\n",
       "      <td>0.675107</td>\n",
       "      <td>0.679593</td>\n",
       "      <td>0.680178</td>\n",
       "      <td>0.689588</td>\n",
       "      <td>0.691806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848174</td>\n",
       "      <td>0.852419</td>\n",
       "      <td>0.855621</td>\n",
       "      <td>0.859238</td>\n",
       "      <td>0.862404</td>\n",
       "      <td>0.865251</td>\n",
       "      <td>0.867542</td>\n",
       "      <td>0.870943</td>\n",
       "      <td>0.874038</td>\n",
       "      <td>0.876732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17669</td>\n",
       "      <td>0</td>\n",
       "      <td>carobka.su классик ваз тольятт дааз вис задн с...</td>\n",
       "      <td>0.723980</td>\n",
       "      <td>0.739090</td>\n",
       "      <td>0.792261</td>\n",
       "      <td>0.815722</td>\n",
       "      <td>0.819343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848174</td>\n",
       "      <td>0.852419</td>\n",
       "      <td>0.855621</td>\n",
       "      <td>0.859238</td>\n",
       "      <td>0.862404</td>\n",
       "      <td>0.865251</td>\n",
       "      <td>0.867542</td>\n",
       "      <td>0.870943</td>\n",
       "      <td>0.874038</td>\n",
       "      <td>0.876732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>14852</td>\n",
       "      <td>0</td>\n",
       "      <td>cartore.ru ступиц нив — зам подшипник сво рук ...</td>\n",
       "      <td>0.459298</td>\n",
       "      <td>0.489671</td>\n",
       "      <td>0.522485</td>\n",
       "      <td>0.530556</td>\n",
       "      <td>0.544479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848174</td>\n",
       "      <td>0.852419</td>\n",
       "      <td>0.855621</td>\n",
       "      <td>0.859238</td>\n",
       "      <td>0.862404</td>\n",
       "      <td>0.865251</td>\n",
       "      <td>0.867542</td>\n",
       "      <td>0.870943</td>\n",
       "      <td>0.874038</td>\n",
       "      <td>0.876732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11684</th>\n",
       "      <td>11686</td>\n",
       "      <td>129</td>\n",
       "      <td>26672</td>\n",
       "      <td>0</td>\n",
       "      <td>eva.ru ❤★✿★апрелят ❤★✿ врем дат сообщен ответ ...</td>\n",
       "      <td>0.874844</td>\n",
       "      <td>0.903136</td>\n",
       "      <td>0.912564</td>\n",
       "      <td>0.949960</td>\n",
       "      <td>0.958090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938553</td>\n",
       "      <td>0.942205</td>\n",
       "      <td>0.944848</td>\n",
       "      <td>0.947486</td>\n",
       "      <td>0.949626</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.953855</td>\n",
       "      <td>0.955754</td>\n",
       "      <td>0.957495</td>\n",
       "      <td>0.959603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11685</th>\n",
       "      <td>11687</td>\n",
       "      <td>129</td>\n",
       "      <td>25838</td>\n",
       "      <td>0</td>\n",
       "      <td>psychotherapie-ros.at gastebuch http cu url bi...</td>\n",
       "      <td>0.872837</td>\n",
       "      <td>0.889652</td>\n",
       "      <td>0.919874</td>\n",
       "      <td>0.933882</td>\n",
       "      <td>0.937706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938553</td>\n",
       "      <td>0.942205</td>\n",
       "      <td>0.944848</td>\n",
       "      <td>0.947486</td>\n",
       "      <td>0.949626</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.953855</td>\n",
       "      <td>0.955754</td>\n",
       "      <td>0.957495</td>\n",
       "      <td>0.959603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11686</th>\n",
       "      <td>11688</td>\n",
       "      <td>129</td>\n",
       "      <td>25703</td>\n",
       "      <td>0</td>\n",
       "      <td>tumblr.com jizolofej archiv эт котор класс сам...</td>\n",
       "      <td>0.761570</td>\n",
       "      <td>0.800528</td>\n",
       "      <td>0.822553</td>\n",
       "      <td>0.823066</td>\n",
       "      <td>0.834615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938553</td>\n",
       "      <td>0.942205</td>\n",
       "      <td>0.944848</td>\n",
       "      <td>0.947486</td>\n",
       "      <td>0.949626</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.953855</td>\n",
       "      <td>0.955754</td>\n",
       "      <td>0.957495</td>\n",
       "      <td>0.959603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11687</th>\n",
       "      <td>11689</td>\n",
       "      <td>129</td>\n",
       "      <td>27885</td>\n",
       "      <td>0</td>\n",
       "      <td>prodeundi.ru как зовут парн диа шурыгин пуст г...</td>\n",
       "      <td>0.239602</td>\n",
       "      <td>0.908111</td>\n",
       "      <td>0.932815</td>\n",
       "      <td>0.933190</td>\n",
       "      <td>0.938295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938553</td>\n",
       "      <td>0.942205</td>\n",
       "      <td>0.944848</td>\n",
       "      <td>0.947486</td>\n",
       "      <td>0.949626</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.953855</td>\n",
       "      <td>0.955754</td>\n",
       "      <td>0.957495</td>\n",
       "      <td>0.959603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11688</th>\n",
       "      <td>11690</td>\n",
       "      <td>129</td>\n",
       "      <td>27987</td>\n",
       "      <td>0</td>\n",
       "      <td>smiexpress.ru культур шоубиз виз ревизорр смол...</td>\n",
       "      <td>0.906673</td>\n",
       "      <td>0.908234</td>\n",
       "      <td>0.910502</td>\n",
       "      <td>0.916759</td>\n",
       "      <td>0.931809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938553</td>\n",
       "      <td>0.942205</td>\n",
       "      <td>0.944848</td>\n",
       "      <td>0.947486</td>\n",
       "      <td>0.949626</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.953855</td>\n",
       "      <td>0.955754</td>\n",
       "      <td>0.957495</td>\n",
       "      <td>0.959603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11689 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pair_id  group_id  doc_id  target  \\\n",
       "0            1         1   15731       0   \n",
       "1            2         1   14829       0   \n",
       "2            3         1   15764       0   \n",
       "3            4         1   17669       0   \n",
       "4            5         1   14852       0   \n",
       "...        ...       ...     ...     ...   \n",
       "11684    11686       129   26672       0   \n",
       "11685    11687       129   25838       0   \n",
       "11686    11688       129   25703       0   \n",
       "11687    11689       129   27885       0   \n",
       "11688    11690       129   27987       0   \n",
       "\n",
       "                                                   words      fit0      fit1  \\\n",
       "0      automn.ru ваз зам подшипник ступиц нив подшипн...  0.112565  0.128736   \n",
       "1      tiu.ru ваз опт соч сравн цен куп потребительск...  0.361563  0.367733   \n",
       "2      drom.ru куп ступиц лад калин трансмисс переход...  0.675107  0.679593   \n",
       "3      carobka.su классик ваз тольятт дааз вис задн с...  0.723980  0.739090   \n",
       "4      cartore.ru ступиц нив — зам подшипник сво рук ...  0.459298  0.489671   \n",
       "...                                                  ...       ...       ...   \n",
       "11684  eva.ru ❤★✿★апрелят ❤★✿ врем дат сообщен ответ ...  0.874844  0.903136   \n",
       "11685  psychotherapie-ros.at gastebuch http cu url bi...  0.872837  0.889652   \n",
       "11686  tumblr.com jizolofej archiv эт котор класс сам...  0.761570  0.800528   \n",
       "11687  prodeundi.ru как зовут парн диа шурыгин пуст г...  0.239602  0.908111   \n",
       "11688  smiexpress.ru культур шоубиз виз ревизорр смол...  0.906673  0.908234   \n",
       "\n",
       "           fit2      fit3      fit4  ...    fit115    fit116    fit117  \\\n",
       "0      0.303646  0.381200  0.515635  ...  0.848174  0.852419  0.855621   \n",
       "1      0.445630  0.476053  0.587270  ...  0.848174  0.852419  0.855621   \n",
       "2      0.680178  0.689588  0.691806  ...  0.848174  0.852419  0.855621   \n",
       "3      0.792261  0.815722  0.819343  ...  0.848174  0.852419  0.855621   \n",
       "4      0.522485  0.530556  0.544479  ...  0.848174  0.852419  0.855621   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "11684  0.912564  0.949960  0.958090  ...  0.938553  0.942205  0.944848   \n",
       "11685  0.919874  0.933882  0.937706  ...  0.938553  0.942205  0.944848   \n",
       "11686  0.822553  0.823066  0.834615  ...  0.938553  0.942205  0.944848   \n",
       "11687  0.932815  0.933190  0.938295  ...  0.938553  0.942205  0.944848   \n",
       "11688  0.910502  0.916759  0.931809  ...  0.938553  0.942205  0.944848   \n",
       "\n",
       "         fit118    fit119    fit120    fit121    fit122    fit123    fit124  \n",
       "0      0.859238  0.862404  0.865251  0.867542  0.870943  0.874038  0.876732  \n",
       "1      0.859238  0.862404  0.865251  0.867542  0.870943  0.874038  0.876732  \n",
       "2      0.859238  0.862404  0.865251  0.867542  0.870943  0.874038  0.876732  \n",
       "3      0.859238  0.862404  0.865251  0.867542  0.870943  0.874038  0.876732  \n",
       "4      0.859238  0.862404  0.865251  0.867542  0.870943  0.874038  0.876732  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "11684  0.947486  0.949626  0.951807  0.953855  0.955754  0.957495  0.959603  \n",
       "11685  0.947486  0.949626  0.951807  0.953855  0.955754  0.957495  0.959603  \n",
       "11686  0.947486  0.949626  0.951807  0.953855  0.955754  0.957495  0.959603  \n",
       "11687  0.947486  0.949626  0.951807  0.953855  0.955754  0.957495  0.959603  \n",
       "11688  0.947486  0.949626  0.951807  0.953855  0.955754  0.957495  0.959603  \n",
       "\n",
       "[11689 rows x 130 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>words</th>\n",
       "      <th>fit0</th>\n",
       "      <th>fit1</th>\n",
       "      <th>fit2</th>\n",
       "      <th>fit3</th>\n",
       "      <th>fit4</th>\n",
       "      <th>fit5</th>\n",
       "      <th>...</th>\n",
       "      <th>fit116</th>\n",
       "      <th>fit117</th>\n",
       "      <th>fit118</th>\n",
       "      <th>fit119</th>\n",
       "      <th>fit120</th>\n",
       "      <th>fit121</th>\n",
       "      <th>fit122</th>\n",
       "      <th>fit123</th>\n",
       "      <th>fit124</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11691</td>\n",
       "      <td>130</td>\n",
       "      <td>6710</td>\n",
       "      <td>youtube.com как прописа админк в кс себ ил дру...</td>\n",
       "      <td>0.243496</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.426603</td>\n",
       "      <td>0.429909</td>\n",
       "      <td>0.439519</td>\n",
       "      <td>0.472278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838425</td>\n",
       "      <td>0.842890</td>\n",
       "      <td>0.846824</td>\n",
       "      <td>0.850915</td>\n",
       "      <td>0.854294</td>\n",
       "      <td>0.858193</td>\n",
       "      <td>0.862033</td>\n",
       "      <td>0.865071</td>\n",
       "      <td>0.869393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11692</td>\n",
       "      <td>130</td>\n",
       "      <td>4030</td>\n",
       "      <td>v-sampe.ru скача sgl rp доработк слив мод mysq...</td>\n",
       "      <td>0.398399</td>\n",
       "      <td>0.536240</td>\n",
       "      <td>0.615161</td>\n",
       "      <td>0.651675</td>\n",
       "      <td>0.722984</td>\n",
       "      <td>0.812070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838425</td>\n",
       "      <td>0.842890</td>\n",
       "      <td>0.846824</td>\n",
       "      <td>0.850915</td>\n",
       "      <td>0.854294</td>\n",
       "      <td>0.858193</td>\n",
       "      <td>0.862033</td>\n",
       "      <td>0.865071</td>\n",
       "      <td>0.869393</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11693</td>\n",
       "      <td>130</td>\n",
       "      <td>5561</td>\n",
       "      <td>dream-x.ru как прописа админк кс count strik к...</td>\n",
       "      <td>0.474139</td>\n",
       "      <td>0.519859</td>\n",
       "      <td>0.522003</td>\n",
       "      <td>0.540727</td>\n",
       "      <td>0.561512</td>\n",
       "      <td>0.575746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838425</td>\n",
       "      <td>0.842890</td>\n",
       "      <td>0.846824</td>\n",
       "      <td>0.850915</td>\n",
       "      <td>0.854294</td>\n",
       "      <td>0.858193</td>\n",
       "      <td>0.862033</td>\n",
       "      <td>0.865071</td>\n",
       "      <td>0.869393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11694</td>\n",
       "      <td>130</td>\n",
       "      <td>4055</td>\n",
       "      <td>net.ru как прописа прост админк кс админк кс п...</td>\n",
       "      <td>0.397117</td>\n",
       "      <td>0.429909</td>\n",
       "      <td>0.475111</td>\n",
       "      <td>0.487070</td>\n",
       "      <td>0.509731</td>\n",
       "      <td>0.533393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838425</td>\n",
       "      <td>0.842890</td>\n",
       "      <td>0.846824</td>\n",
       "      <td>0.850915</td>\n",
       "      <td>0.854294</td>\n",
       "      <td>0.858193</td>\n",
       "      <td>0.862033</td>\n",
       "      <td>0.865071</td>\n",
       "      <td>0.869393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11695</td>\n",
       "      <td>130</td>\n",
       "      <td>4247</td>\n",
       "      <td>o3one.ru подбор админ сервер код арх форум ozo...</td>\n",
       "      <td>0.650088</td>\n",
       "      <td>0.691921</td>\n",
       "      <td>0.720278</td>\n",
       "      <td>0.733805</td>\n",
       "      <td>0.762663</td>\n",
       "      <td>0.766120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838425</td>\n",
       "      <td>0.842890</td>\n",
       "      <td>0.846824</td>\n",
       "      <td>0.850915</td>\n",
       "      <td>0.854294</td>\n",
       "      <td>0.858193</td>\n",
       "      <td>0.862033</td>\n",
       "      <td>0.865071</td>\n",
       "      <td>0.869393</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16622</th>\n",
       "      <td>28313</td>\n",
       "      <td>309</td>\n",
       "      <td>16637</td>\n",
       "      <td>mail.ru ответ mail ru полезн куша творог утр х...</td>\n",
       "      <td>0.567072</td>\n",
       "      <td>0.595642</td>\n",
       "      <td>0.633593</td>\n",
       "      <td>0.639789</td>\n",
       "      <td>0.656616</td>\n",
       "      <td>0.678725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591068</td>\n",
       "      <td>0.596888</td>\n",
       "      <td>0.602179</td>\n",
       "      <td>0.608353</td>\n",
       "      <td>0.614358</td>\n",
       "      <td>0.620099</td>\n",
       "      <td>0.624341</td>\n",
       "      <td>0.629072</td>\n",
       "      <td>0.635117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16623</th>\n",
       "      <td>28314</td>\n",
       "      <td>309</td>\n",
       "      <td>16759</td>\n",
       "      <td>inmoment.ru творог полезн свойств лечен творог...</td>\n",
       "      <td>0.471898</td>\n",
       "      <td>0.538731</td>\n",
       "      <td>0.547366</td>\n",
       "      <td>0.557075</td>\n",
       "      <td>0.562548</td>\n",
       "      <td>0.563435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591068</td>\n",
       "      <td>0.596888</td>\n",
       "      <td>0.602179</td>\n",
       "      <td>0.608353</td>\n",
       "      <td>0.614358</td>\n",
       "      <td>0.620099</td>\n",
       "      <td>0.624341</td>\n",
       "      <td>0.629072</td>\n",
       "      <td>0.635117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16624</th>\n",
       "      <td>28315</td>\n",
       "      <td>309</td>\n",
       "      <td>15358</td>\n",
       "      <td>edaplus.info творог полезн опасн свойств творо...</td>\n",
       "      <td>0.416179</td>\n",
       "      <td>0.437015</td>\n",
       "      <td>0.446254</td>\n",
       "      <td>0.456509</td>\n",
       "      <td>0.458961</td>\n",
       "      <td>0.471898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591068</td>\n",
       "      <td>0.596888</td>\n",
       "      <td>0.602179</td>\n",
       "      <td>0.608353</td>\n",
       "      <td>0.614358</td>\n",
       "      <td>0.620099</td>\n",
       "      <td>0.624341</td>\n",
       "      <td>0.629072</td>\n",
       "      <td>0.635117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16625</th>\n",
       "      <td>28316</td>\n",
       "      <td>309</td>\n",
       "      <td>17287</td>\n",
       "      <td>mail.ru ответ mail ru чем полез творог творог ...</td>\n",
       "      <td>0.130777</td>\n",
       "      <td>0.201235</td>\n",
       "      <td>0.312665</td>\n",
       "      <td>0.335397</td>\n",
       "      <td>0.340588</td>\n",
       "      <td>0.379820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591068</td>\n",
       "      <td>0.596888</td>\n",
       "      <td>0.602179</td>\n",
       "      <td>0.608353</td>\n",
       "      <td>0.614358</td>\n",
       "      <td>0.620099</td>\n",
       "      <td>0.624341</td>\n",
       "      <td>0.629072</td>\n",
       "      <td>0.635117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16626</th>\n",
       "      <td>28317</td>\n",
       "      <td>309</td>\n",
       "      <td>16026</td>\n",
       "      <td>jv.ru творог польз вред как выбира продукт jv ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.557407</td>\n",
       "      <td>0.558617</td>\n",
       "      <td>0.582195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591068</td>\n",
       "      <td>0.596888</td>\n",
       "      <td>0.602179</td>\n",
       "      <td>0.608353</td>\n",
       "      <td>0.614358</td>\n",
       "      <td>0.620099</td>\n",
       "      <td>0.624341</td>\n",
       "      <td>0.629072</td>\n",
       "      <td>0.635117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16627 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pair_id  group_id  doc_id  \\\n",
       "0        11691       130    6710   \n",
       "1        11692       130    4030   \n",
       "2        11693       130    5561   \n",
       "3        11694       130    4055   \n",
       "4        11695       130    4247   \n",
       "...        ...       ...     ...   \n",
       "16622    28313       309   16637   \n",
       "16623    28314       309   16759   \n",
       "16624    28315       309   15358   \n",
       "16625    28316       309   17287   \n",
       "16626    28317       309   16026   \n",
       "\n",
       "                                                   words      fit0      fit1  \\\n",
       "0      youtube.com как прописа админк в кс себ ил дру...  0.243496  0.395833   \n",
       "1      v-sampe.ru скача sgl rp доработк слив мод mysq...  0.398399  0.536240   \n",
       "2      dream-x.ru как прописа админк кс count strik к...  0.474139  0.519859   \n",
       "3      net.ru как прописа прост админк кс админк кс п...  0.397117  0.429909   \n",
       "4      o3one.ru подбор админ сервер код арх форум ozo...  0.650088  0.691921   \n",
       "...                                                  ...       ...       ...   \n",
       "16622  mail.ru ответ mail ru полезн куша творог утр х...  0.567072  0.595642   \n",
       "16623  inmoment.ru творог полезн свойств лечен творог...  0.471898  0.538731   \n",
       "16624  edaplus.info творог полезн опасн свойств творо...  0.416179  0.437015   \n",
       "16625  mail.ru ответ mail ru чем полез творог творог ...  0.130777  0.201235   \n",
       "16626  jv.ru творог польз вред как выбира продукт jv ...  0.000000  0.000000   \n",
       "\n",
       "           fit2      fit3      fit4      fit5  ...    fit116    fit117  \\\n",
       "0      0.426603  0.429909  0.439519  0.472278  ...  0.838425  0.842890   \n",
       "1      0.615161  0.651675  0.722984  0.812070  ...  0.838425  0.842890   \n",
       "2      0.522003  0.540727  0.561512  0.575746  ...  0.838425  0.842890   \n",
       "3      0.475111  0.487070  0.509731  0.533393  ...  0.838425  0.842890   \n",
       "4      0.720278  0.733805  0.762663  0.766120  ...  0.838425  0.842890   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "16622  0.633593  0.639789  0.656616  0.678725  ...  0.591068  0.596888   \n",
       "16623  0.547366  0.557075  0.562548  0.563435  ...  0.591068  0.596888   \n",
       "16624  0.446254  0.456509  0.458961  0.471898  ...  0.591068  0.596888   \n",
       "16625  0.312665  0.335397  0.340588  0.379820  ...  0.591068  0.596888   \n",
       "16626  0.000000  0.557407  0.558617  0.582195  ...  0.591068  0.596888   \n",
       "\n",
       "         fit118    fit119    fit120    fit121    fit122    fit123    fit124  \\\n",
       "0      0.846824  0.850915  0.854294  0.858193  0.862033  0.865071  0.869393   \n",
       "1      0.846824  0.850915  0.854294  0.858193  0.862033  0.865071  0.869393   \n",
       "2      0.846824  0.850915  0.854294  0.858193  0.862033  0.865071  0.869393   \n",
       "3      0.846824  0.850915  0.854294  0.858193  0.862033  0.865071  0.869393   \n",
       "4      0.846824  0.850915  0.854294  0.858193  0.862033  0.865071  0.869393   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "16622  0.602179  0.608353  0.614358  0.620099  0.624341  0.629072  0.635117   \n",
       "16623  0.602179  0.608353  0.614358  0.620099  0.624341  0.629072  0.635117   \n",
       "16624  0.602179  0.608353  0.614358  0.620099  0.624341  0.629072  0.635117   \n",
       "16625  0.602179  0.608353  0.614358  0.620099  0.624341  0.629072  0.635117   \n",
       "16626  0.602179  0.608353  0.614358  0.620099  0.624341  0.629072  0.635117   \n",
       "\n",
       "       target  \n",
       "0           1  \n",
       "1           0  \n",
       "2           1  \n",
       "3           1  \n",
       "4           0  \n",
       "...       ...  \n",
       "16622       1  \n",
       "16623       1  \n",
       "16624       1  \n",
       "16625       1  \n",
       "16626       1  \n",
       "\n",
       "[16627 rows x 130 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['pair_id', 'target']].to_csv('new_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
